#!/usr/bin/python -u
#team dan_and_will

import re
import sys

from http import http, E404

csrf_regex = re.compile(r"name='csrfmiddlewaretoken' value='[0-9a-f]{32}'")
profile_regex = re.compile(r"\/fakebook\/\d+\/")
secret_key_regex = re.compile(r"<h2 class='secret_flag' style=\"color:red\">FLAG: [A-z\d]{64}<\/h2>")

class webcrawler:

    def __init__(self, uname, pwd):
        """seup the crawler
        """
        self.uname = uname
        self.pwd = pwd
        self.http = http('fring.ccs.neu.edu')
        self.seen_profs = set()
        self.queue = set()
        self.skeys = set()

    def find_flags(self, page, u):
        """look for flags of profile page
        """
        flags = secret_key_regex.findall(page)
        if flags:
            self.skeys.add(flags[0][-69:-5])

    def find_profiles(self, page):
        """look for other profiles on friends list
        """
        profs = set(profile_regex.findall(page)) - self.seen_profs
        self.queue.update(profs)

    def login(self):
        """handle login form
        """
        login_page = self.http.get('/fakebook/')
        csrf_token = csrf_regex.findall(login_page)[0][-33:-1]
        form_data = "username={uname}&password={pwd}&csrfmiddlewaretoken={csrf}&next=%2Ffakebook%2F".format(uname=self.uname, pwd=self.pwd, csrf=csrf_token)
        status, redir = self.http.post("/accounts/login/", form_data)
        if not (status == 302 and redir == '/fakebook/'):
            raise Exception('You suck at loging in')

    def crawl_profile(self, prof_url):
        """look for flags and friends on a profile
        """
        profile = self.http.get(prof_url)
        self.find_flags(profile, prof_url)
        page_id = 1
        while True:
            try:
                friends_page = self.http.get(prof_url + 'friends/{id}/'.format(id=page_id))
            except E404:
                break
            self.find_flags(friends_page, prof_url + 'friends/{id}/'.format(id=page_id))
            self.find_profiles(friends_page)
            page_id += 1

    def crawl_site(self):
        """handle scrawling all found profiles
        """
        home_page = self.http.get('/fakebook/');
        self.find_profiles(home_page)
        while(len(self.skeys) < 5):
            next_prof = self.queue.pop()
            self.seen_profs.add(next_prof)
            self.crawl_profile(next_prof)

    def run(self):
        """handle running logger and printing flags
        """
        self.login()
        self.crawl_site()
        print "\n".join(self.skeys)

if __name__ == '__main__':
    """read command line args and run crawler
    """
    wc = webcrawler(sys.argv[1], sys.argv[2])
    wc.run()
